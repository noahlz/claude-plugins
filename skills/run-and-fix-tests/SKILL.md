---
name: run-and-fix-tests
description: Build the project, run tests and systematically fix any failures. Activate when user says phrases such as "run tests", "test", "build and test" or "fix tests".
---

## 0. Resolve Plugin Root

â†’ Resolve plugin root environment (check local project first, then user home):
```bash
RESOLVER=""
if [ -x "./.claude/resolve_plugin_root.sh" ]; then
  RESOLVER="./.claude/resolve_plugin_root.sh"
elif [ -x "$HOME/.claude/resolve_plugin_root.sh" ]; then
  RESOLVER="$HOME/.claude/resolve_plugin_root.sh"
else
  echo "Error: resolve_plugin_root.sh not found in ./.claude/ or $HOME/.claude/" >&2
  exit 1
fi
CLAUDE_PLUGIN_ROOT="$($RESOLVER "dev-workflow@noahlz.github.io")" || { echo "Error: Failed to resolve plugin root" >&2; exit 1; }
export CLAUDE_PLUGIN_ROOT
```

âœ“ Plugin root resolved â†’ Proceed to step 1 (Detect Build Configuration)

## 1. Detect Build Configuration

â†’ Check if `.claude/settings.plugins.run-and-fix-tests.json` exists
âœ“ Config exists â†’ Proceed to step 2
âœ— Config missing â†’ Run detection and auto-config:

â†’ Source: `${CLAUDE_PLUGIN_ROOT}/skills/run-and-fix-tests/scripts/detect-and-resolve.sh`
  - Scans project for build tool config files (package.json, pom.xml, build.gradle, etc.)
  - Detects which tools are present
  - Automatically selects and applies appropriate default configuration

â†’ Auto-selection rules:
  - Exactly 1 tool detected â†’ Use `defaults/{tool}.json`
  - Multiple tools in different locations â†’ Generate polyglot config (ðŸ”§ shown to user)
  - Multiple tools in same location â†’ Generate polyglot config
  - No matching default exists â†’ Use `example.json` placeholder template (user must customize)
  - 0 tools detected â†’ Error: no build tools detected

âœ“ Config created successfully â†’ Proceed to step 2
âœ— No tools detected â†’ Error, user must create `.claude/settings.plugins.run-and-fix-tests.json` manually
âœ— Using placeholder config â†’ User must edit `.claude/settings.plugins.run-and-fix-tests.json` before step 2

## 2. Load Configuration

â†’ Execute load-config script to output configuration as eval-able statements:
```bash
eval "$(${CLAUDE_PLUGIN_ROOT}/skills/run-and-fix-tests/scripts/load-config.sh "${CLAUDE_PLUGIN_ROOT}")"
```
âœ— Script fails â†’ Display error and stop
âœ“ Script succeeds â†’ Environment variables set:
  - BUILD_CMD, BUILD_LOG, BUILD_ERROR_PATTERN, BUILD_WORKING_DIR
  - TEST_CMD, TEST_LOG, TEST_ERROR_PATTERN
  - TEST_SINGLE_CMD, TEST_SINGLE_LOG, TEST_SINGLE_ERROR_PATTERN
  - LOG_DIR (tool-specific, e.g., dist/, build/, target/)
  - BUILD_MULTI (true if multi-build, false if single)

â†’ Check command argument: `TEST_FILE="$1"`
â†’ Determine mode:
  - `$TEST_FILE` not empty â†’ Single test mode
  - `$TEST_FILE` empty â†’ All tests mode

â†’ Store initial working directory: `INITIAL_PWD=$(pwd)`

## 3. Build Project

â†’ Create log directory: `mkdir -p "$LOG_DIR"`
â†’ Check build type: `$BUILD_MULTI`

**Single Build:**
â†’ Change to build working directory: `cd "$BUILD_WORKING_DIR"`
â†’ Execute build command silently to log file: `$BUILD_CMD > "$BUILD_LOG" 2>&1`
âœ“ Exit 0 â†’ Return to INITIAL_PWD, proceed to step 4 (Run Tests)
âœ— Exit non-zero â†’ Return to INITIAL_PWD, proceed to step 3a (Extract Build Errors)

**Multi-Build:**
â†’ For each build in detected tools:
  â†’ Change to build working directory
  â†’ Execute build command silently to log file: `$BUILD_CMD > "$BUILD_LOG" 2>&1`
  â†’ On success: continue to next build
  â†’ On failure: return to INITIAL_PWD, proceed to step 3a (Extract Build Errors)

âœ“ All builds succeed â†’ Return to INITIAL_PWD, proceed to step 4 (Run Tests)

## 3a. Extract Build Errors

â†’ Try to get language diagnostics from editor using mcp__ide__getDiagnostics tool
âœ“ Editor diagnostics available â†’ Parse diagnostics JSON for:
  - File paths with errors
  - Line numbers and column positions
  - Error messages and severity
  - Error codes (if available)
  â†’ Display compilation errors to user with file:line references
âœ— Editor diagnostics not available or empty â†’ Proceed to log parsing

â†’ Parse build log at `$BUILD_LOG` using regex from `$BUILD_ERROR_PATTERN`
â†’ Extract up to 30 distinct compilation errors with:
  - File paths
  - Line numbers (if present in log)
  - Error messages
â†’ Display compilation error summary to user

â†’ Use AskUserQuestion: "Build failed with [N] compilation errors. Fix them?"
  - "Yes" â†’ Proceed to step 3b
  - "No" â†’ Stop

## 3b. Fix Compilation Errors

â†’ For each compilation error identified:
  â†’ Read the file with the error
  â†’ Identify the compilation issue (syntax error, type error, missing import, etc.)
  â†’ Implement fix to the source code
  â†’ Mark error as addressed

â†’ When all errors are addressed, return to step 3 (Build Project)

## 4. Run Tests

â†’ Determine test command based on mode:
  - Single test mode: TEST_CMD = `$TEST_SINGLE_CMD` with {testFile} replaced
  - All tests mode: TEST_CMD = `$TEST_CMD`

â†’ Change to test working directory (if different from build dir)
â†’ Execute test command silently to log file: `$TEST_CMD > "$TEST_LOG" 2>&1`
âœ“ Exit 0 â†’ Return to INITIAL_PWD, all tests pass, proceed to step 9 (Success)
âœ— Exit non-zero â†’ Return to INITIAL_PWD, tests failed, proceed to step 5 (Extract Test Errors)

## 5. Extract Test Errors

â†’ Parse test log at `$TEST_LOG` to identify failing tests
â†’ Extract error patterns from log using `$TEST_ERROR_PATTERN` regex
â†’ Identify up to 30 distinct test failures
â†’ Display error summary to user with:
  - List of failing test names/paths
  - Error messages and relevant output from test log
  - Stack traces (if available)
â†’ Proceed to step 6 (Create Fix Plan)

## 6. Create Fix Plan

â†’ Analyze extracted failures to identify distinct failing tests
â†’ Use TodoWrite to create todo list with one item per failing test:
  - content: "Fix [test name]"
  - activeForm: "Fixing [test name]"
  - status: "pending"
â†’ Proceed to step 7 (Ask to Fix Tests)

## 7. Ask to Fix Tests

â†’ Use AskUserQuestion:
  - "Start fixing tests one by one?" (recommended)
  - "No, I'll fix manually"
  - "Other"

âœ“ User confirms â†’ Proceed to step 8 (Fix Tests Iteratively)
âœ— User declines â†’ Stop

## 8. Fix Tests Iteratively

â†’ Get next pending test from todo list
â†’ Mark test as "in_progress" using TodoWrite
â†’ Initialize retry counter: `RETRY_COUNT=0`

### 8a. Attempt Fix (Iterate up to 3 times)

â†’ Increment `RETRY_COUNT`
â†’ Read failing test file and implementation file
â†’ Implement fix to source code
â†’ Run specific single test silently to verify fix:
  - Command: `$TEST_SINGLE_CMD > "$TEST_SINGLE_LOG" 2>&1` with test file/class name
  - Capture exit code from command execution
â†’ Display result to user

âœ“ Test passes (exit code 0) â†’ Mark todo as "completed", proceed to step 8b
âœ— Test still fails (exit code non-zero):
  - If `RETRY_COUNT < 3` â†’ Display failure reason, use AskUserQuestion: "Try again?"
    - "Yes" â†’ Return to step 8a (Attempt Fix again)
    - "No" â†’ Skip this test and proceed to step 8b
  - If `RETRY_COUNT == 3` â†’ Display "Attempted fix 3 times without success"
    â†’ Use AskUserQuestion: "Continue trying to fix this test?"
      - "Yes, keep trying" â†’ Continue from step 8a (increment counter)
      - "No, skip it" â†’ Proceed to step 8b
      - "No, stop for now" â†’ Stop

### 8b. Move to Next Test

â†’ Use AskUserQuestion:
  - "Fix next test?"
  - "Re-run all tests?" (clear todos, return to step 4)
  - "Stop for now" â†’ Stop
  - "Other"

âœ“ "Fix next test" â†’ If tests remain, return to step 8; else proceed to step 4 (Run Tests)
âœ“ "Re-run all tests" â†’ Clear todos with TodoWrite, return to step 4 (Run Tests)
âœ— "Stop for now" â†’ Stop

## 9. Success

âœ… All tests passed
â†’ Clear todo list with TodoWrite (empty)
â†’ Display success message with log file locations

---

**âš ï¸  CRITICAL EXECUTION RULES**

- **Mandatory flow**: After step 5 (Extract Test Errors) and step 6 (Create Fix Plan), ALWAYS proceed to step 7 (Ask to Fix Tests). Do NOT stop or skip to user.
- **User confirmation required**: ALWAYS ask user via AskUserQuestion in step 7 before proceeding to step 8. Only proceed to step 8 if user confirms.
- **Silent execution**: NEVER use `tee` when running build or test commands. Redirect all output to log files (`> "$LOG_FILE" 2>&1`). Only inspect logs when command returns non-zero exit code.
- **Exit code checking**: Always capture and check exit codes. Zero = success, non-zero = failure.
- **No assumptions**: Never assume errors are "pre-existing" or skip investigating them. All errors must be analyzed unless user explicitly stops the workflow.
