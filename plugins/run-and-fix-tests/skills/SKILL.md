---
name: run-and-fix-tests
description: Build project and run tests with clean output, fix any failures. Activate when user says "run tests", "test", "build and test", "fix tests", or "make test".
---

## 1. Load Configuration

â†’ Run: `source ${CLAUDE_PLUGIN_ROOT}/scripts/load-config.sh`
âœ— Script fails â†’ Display error and stop
âœ“ Configuration loaded and merged
â†’ Check command arguments: `TEST_FILE="$1"`
â†’ Determine mode:
  - If `$TEST_FILE` is set â†’ Single test mode
  - If `$TEST_FILE` is empty â†’ All tests mode
â†’ Use in subsequent steps:
  - `$BUILD_CMD` - Build command (default: `npm run build`)
  - `$BUILD_LOG` - Build log file (default: `dist/build.log`)
  - `$BUILD_ERROR_PATTERN` - Build error regex (default: `(error|Error|âœ˜)`)
  - `$TEST_CMD` - All tests command (default: `npm test`)
  - `$TEST_LOG` - All tests log file (default: `dist/test.log`)
  - `$TEST_ERROR_PATTERN` - Test error regex (default: `(FAIL|â—|Error:|Expected|Received)`)
  - `$TEST_SINGLE_CMD` - Single test command template (default: `npm test -- {testFile}`)
  - `$TEST_SINGLE_LOG` - Single test log file (default: `dist/test-single.log`)
  - `$TEST_SINGLE_ERROR_PATTERN` - Single test error regex (default: same as all tests)
  - `$LOG_DIR` - Log directory (default: `dist`)
  - `$TEST_FILE` - Test file argument from command (from `$1`)

## 2. Build Project

â†’ Create log directory: `mkdir -p "$LOG_DIR"`
â†’ Run build silently: `$BUILD_CMD > "$BUILD_LOG" 2>&1`
â†’ Check exit code

âœ“ Build succeeded (exit 0)
  â†’ Proceed to step 3

âœ— Build failed (exit non-zero)
  â†’ Extract errors: `grep -E "$BUILD_ERROR_PATTERN" "$BUILD_LOG" | head -20 || echo "No errors matched pattern"`
  â†’ Display: "âŒ Build failed:" + errors
  â†’ Display: "ğŸ“ Full log: $BUILD_LOG"
  â†’ Ask user via AskUserQuestion: "Build failed. Should I analyze and fix the build issues?"
    - "Yes" (recommended) â†’ Analyze and fix, return to step 2
    - "No, I'll fix manually" â†’ Stop, user will fix
    - "Other" â†’ Follow custom instruction

## 3. Run Tests

â†’ Determine test command based on mode:
  - Single test mode: Replace `{testFile}` placeholder in `$TEST_SINGLE_CMD` with `$TEST_FILE`
    - `ACTUAL_CMD=$(echo "$TEST_SINGLE_CMD" | sed "s|{testFile}|${TEST_FILE}|g")`
    - `ACTUAL_LOG="$TEST_SINGLE_LOG"`
    - `ACTUAL_PATTERN="$TEST_SINGLE_ERROR_PATTERN"`
  - All tests mode:
    - `ACTUAL_CMD="$TEST_CMD"`
    - `ACTUAL_LOG="$TEST_LOG"`
    - `ACTUAL_PATTERN="$TEST_ERROR_PATTERN"`

â†’ Display: "ğŸ§ª Running tests..." + (single test mode: " (${TEST_FILE})" or all tests mode: " (all tests)")
â†’ Run tests silently: `$ACTUAL_CMD > "$ACTUAL_LOG" 2>&1`
â†’ Check exit code

âœ“ Tests passed (exit 0)
  â†’ Proceed to step 9

âœ— Tests failed (exit non-zero)
  â†’ Proceed to step 4

## 4. Extract Errors

â†’ Extract errors: `grep -E "$ACTUAL_PATTERN" "$ACTUAL_LOG" | head -30 || echo "No errors matched pattern"`
â†’ Count errors: `ERROR_COUNT=$(grep -cE "$ACTUAL_PATTERN" "$ACTUAL_LOG" || echo "0")`
â†’ Display: "âŒ Tests failed: $ERROR_COUNT errors found"
â†’ Display: First 30 matching lines
â†’ Display: "ğŸ“ Full log: $ACTUAL_LOG"
â†’ Proceed to step 5

## 5. Analyze Failures and Create Fix Plan

â†’ Analyze test failures to identify distinct failing tests
â†’ For each failing test, determine:
  - Test name/description
  - Root cause of failure
  - Files that need modification
â†’ Use TodoWrite to create todo list with:
  - One todo per failing test
  - content: "Fix test: [test name]"
  - activeForm: "Fixing test: [test name]"
  - status: "pending" for all tests
â†’ Display: "ğŸ“‹ Created fix plan for N failing tests"
â†’ Proceed to step 6

## 6. Ask to Start Fixing

â†’ Use AskUserQuestion with options:
  - Question: "Start fixing tests one by one?"
  - "Yes" (recommended) â†’ Proceed to step 7
  - "No, I'll fix manually" â†’ Stop, user will fix
  - "Other" â†’ Follow custom instruction

âœ“ User chose "Yes" â†’ Proceed to step 7
âœ— User chose "No, I'll fix manually" â†’ Stop
â†’ User chose "Other" â†’ Follow their custom instruction

## 7. Fix Next Test

â†’ Get next pending test from todo list
â†’ Mark current test as "in_progress" using TodoWrite
â†’ Display: "ğŸ”§ Fixing: [test name]"
â†’ Identify and display files to modify
â†’ Fix issues: modify relevant code files
â†’ Mark current test as "completed" using TodoWrite
â†’ Proceed to step 8

## 8. Ask Next Action

â†’ Count remaining pending tests in todo list
â†’ Use AskUserQuestion with options:
  - Question: "Test fixed! What next? (N tests remaining)"
  - "Fix next test" (recommended if tests remain) â†’ Return to step 7
  - "Re-run all tests" â†’ Clear todo list, return to step 3
  - "Stop for now" â†’ Stop, preserve todo list
  - "Other" â†’ Follow custom instruction

âœ“ User chose "Fix next test" AND tests remain â†’ Return to step 7
âœ“ User chose "Fix next test" AND no tests remain â†’ Display "All tests fixed!", return to step 3
âœ“ User chose "Re-run all tests" â†’ Clear todos with TodoWrite, return to step 3
âœ— User chose "Stop for now" â†’ Stop
â†’ User chose "Other" â†’ Follow their custom instruction

## 9. Success

âœ… All tests passed
âœ… Build complete
â†’ Clear any remaining todos with TodoWrite (empty list)
â†’ Display: "âœ… Success! Build and tests passed"
â†’ Display: "ğŸ“ Build log: $BUILD_LOG"
â†’ Display: "ğŸ“ Test log: $ACTUAL_LOG"
âœ“ Done

ğŸ”§ Configuration: `.claude/build-config.json` (optional project override)
ğŸ“ Default logs: `dist/build.log`, `dist/test.log`, `dist/test-single.log`
