---
name: run-and-fix-tests
description: Build the project, run tests and systematically fix any failures. Activate when user says phrases such as "run tests", "test", "build and test" or "fix tests".
---

## 0. Resolve Plugin Root

â†’ Resolve plugin root environment (check local project first, then user home):
```bash
RESOLVER=""
if [ -x "./.claude/resolve_plugin_root.sh" ]; then
  RESOLVER="./.claude/resolve_plugin_root.sh"
elif [ -x "$HOME/.claude/resolve_plugin_root.sh" ]; then
  RESOLVER="$HOME/.claude/resolve_plugin_root.sh"
else
  echo "Error: resolve_plugin_root.sh not found in ./.claude/ or $HOME/.claude/" >&2
  exit 1
fi
CLAUDE_PLUGIN_ROOT="$($RESOLVER "dev-workflow@noahlz.github.io")" || { echo "Error: Failed to resolve plugin root" >&2; exit 1; }
export CLAUDE_PLUGIN_ROOT
```

âœ“ Plugin root resolved â†’ Proceed to step 1 (Detect Build Configuration)

## 1. Detect Build Configuration

â†’ Check if `.claude/settings.plugins.run-and-fix-tests.json` exists
âœ“ Config exists â†’ Proceed to step 2
âœ— Config missing â†’ Run detection and auto-config:

â†’ Source: `${CLAUDE_PLUGIN_ROOT}/skills/run-and-fix-tests/scripts/detect-and-resolve.sh`
  - Scans project for build tool config files (package.json, pom.xml, build.gradle, etc.)
  - Detects which tools are present
  - Automatically selects and applies appropriate default configuration

â†’ Auto-selection rules:
  - Exactly 1 tool detected â†’ Use `defaults/{tool}.json`
  - Multiple tools in different locations â†’ Generate polyglot config (ðŸ”§ shown to user)
  - Multiple tools in same location â†’ Generate polyglot config
  - No matching default exists â†’ Use `TEMPLATE.json` placeholder template (user must customize)
  - 0 tools detected â†’ Error: no build tools detected

âœ“ Config created successfully â†’ Proceed to step 2
âœ— No tools detected â†’ Error, user must create `.claude/settings.plugins.run-and-fix-tests.json` manually
âœ— Using placeholder config â†’ User must edit `.claude/settings.plugins.run-and-fix-tests.json` before step 2

## 2. Load Configuration

â†’ Execute load-config script to output configuration as eval-able statements:
```bash
eval "$(${CLAUDE_PLUGIN_ROOT}/skills/run-and-fix-tests/scripts/load-config.sh "${CLAUDE_PLUGIN_ROOT}")"
```
âœ— Script fails â†’ Display error and stop
âœ“ Script succeeds â†’ Environment variables set:
  - BUILD_CMD, BUILD_LOG, BUILD_ERROR_PATTERN, BUILD_WORKING_DIR
  - TEST_CMD, TEST_LOG, TEST_ERROR_PATTERN
  - TEST_SINGLE_CMD, TEST_SINGLE_LOG, TEST_SINGLE_ERROR_PATTERN
  - LOG_DIR (tool-specific, e.g., dist/, build/, target/)
  - BUILD_MULTI (true if multi-build, false if single)

â†’ Check command argument: `TEST_FILE="$1"`
â†’ Determine mode:
  - `$TEST_FILE` not empty â†’ Single test mode
  - `$TEST_FILE` empty â†’ All tests mode

â†’ Store initial working directory: `INITIAL_PWD=$(pwd)`

## 3. Build Project

â†’ Create log directory: `mkdir -p "$LOG_DIR"`
â†’ Check build type: `$BUILD_MULTI`

**Single Build:**
â†’ Change to build working directory: `cd "$BUILD_WORKING_DIR"`
â†’ Execute build command silently to log file: `$BUILD_CMD > "$BUILD_LOG" 2>&1`
âœ“ Exit 0 â†’ Return to INITIAL_PWD, proceed to step 4 (Run Tests)
âœ— Exit non-zero â†’ Return to INITIAL_PWD, proceed to step 3a (Extract Build Errors)

**Multi-Build:**
â†’ For each build in detected tools:
  â†’ Change to build working directory
  â†’ Execute build command silently to log file: `$BUILD_CMD > "$BUILD_LOG" 2>&1`
  â†’ On success: continue to next build
  â†’ On failure: return to INITIAL_PWD, proceed to step 3a (Extract Build Errors)

âœ“ All builds succeed â†’ Return to INITIAL_PWD, proceed to step 4 (Run Tests)

## 3a. Extract Build Errors

â†’ Try to get language diagnostics from editor using available IDE MCP or LSP tools
âœ“ MCP or LSP tool available â†’ use it to find and resolve:
  - File paths with errors
  - Line numbers and column positions
  - Error messages and severity
  - Error codes (if available)
  â†’ Display compilation errors to user with file:line references
âœ— Editor diagnostics not available or empty â†’ Proceed to log parsing

â†’ Parse build log at `$BUILD_LOG` using regex from `$BUILD_ERROR_PATTERN`
â†’ Extract up to 30 distinct compilation errors with:
  - File paths
  - Line numbers (if present in log)
  - Error messages
â†’ Display compilation error summary to user

â†’ Use AskUserQuestion: "Build failed with [N] compilation errors. Fix them?"
  - "Yes" â†’ Proceed to step 3b
  - "No" â†’ Stop

## 3b. Fix Compilation Errors

â†’ For each compilation error identified:
  â†’ Read the file with the error
  â†’ Identify the compilation issue (syntax error, type error, missing import, etc.)
  â†’ Implement fix to the source code
  â†’ Mark error as addressed

â†’ When all errors are addressed, return to step 3 (Build Project)

## 4. Run Tests

â†’ Determine test command based on mode:
  - Single test mode: TEST_CMD = `$TEST_SINGLE_CMD` with {testFile} replaced
  - All tests mode: TEST_CMD = `$TEST_CMD`

â†’ Change to test working directory (if different from build dir)
â†’ Execute test command silently to log file: `$TEST_CMD > "$TEST_LOG" 2>&1`
âœ“ Exit 0 â†’ Return to INITIAL_PWD, all tests pass, proceed to step 9 (Success)
âœ— Exit non-zero â†’ Return to INITIAL_PWD, tests failed, proceed to step 5 (Extract Test Errors)

## 5. Extract Test Errors

â†’ Parse test log at `$TEST_LOG` to identify failing tests
â†’ Extract error patterns from log using `$TEST_ERROR_PATTERN` regex
â†’ Identify failing tests (up to 30 distinct failures)

âœ“ 0 failures detected â†’ Proceed to step 9 (Completion)
âœ— 1-30 failures â†’ Display error summary, proceed to step 7
âœ— 30+ failures â†’ Display count, proceed to step 7

â†’ Display error summary to user with:
  - List of failing test names/paths
  - Error messages and relevant output from test log
  - Stack traces (if available)

## 7. Ask to Fix Tests

â†’ Check failure count from step 5:

**If 30+ failures:**
âš ï¸ Display: "30+ tests failed. This is too many for efficient fixing in one chat."
â†’ Use AskUserQuestion:
  - "Attempt to fix 30+ tests?" (not recommended)
  - "No, I'll stop and create a plan"
â†’ If "No" â†’ Stop (user exits to create plan)
â†’ If "Yes" â†’ Continue to step 8

**If 1-29 failures:**
â†’ Use AskUserQuestion:
  - "Start fixing tests?" (recommended)
  - "No, I'll fix manually"
â†’ If "Yes" â†’ Continue to step 8
â†’ If "No" â†’ Stop

## 8. Delegate to Test-Fixer Agent

â†’ Use the `test-fixer` agent to fix failing tests one-by-one.

â†’ Provide agent with context in natural language:
  - Failed test list: [bulleted list with test names and error excerpts from step 5]
  - Example failed test entry: "TestLoginFlow (test/auth.test.js) - Expected 'logged in', got undefined"

â†’ Provide env variable values to agent:
  - TEST_SINGLE_CMD actual value (e.g., "npm test --testNamePattern={testName}")
  - TEST_SINGLE_LOG actual path (e.g., "logs/test-single.log")
  - LOG_DIR actual path (e.g., "logs/")
  - INITIAL_PWD actual path (e.g., "/current/working/directory")

â†’ Agent fixes the tests per its instructions and context provided.

âœ“ Agent completes â†’ Proceed to step 8a

## 8a. Ask User to Re-run Tests

â†’ Use AskUserQuestion:
  - "Re-run all tests to verify fixes?"
  - "No, stop for now"

âœ“ User confirms â†’ Proceed to step 4 (Run Tests)
âœ— User declines â†’ Proceed to step 9

## 9. Completion

â†’ Check if all originally-failing tests were fixed:
  - If yes â†’ Display: "âœ… All tests fixed and passed!"
  - If no â†’ Display: "âš ï¸ Workflow incomplete. Some tests remain unfixed."

â†’ Show summary:
  - Tests fixed in this session
  - Tests skipped/remaining
  - Root causes addressed

â†’ Clear todo list with TodoWrite (empty)
â†’ Exit

---

**âš ï¸  CRITICAL EXECUTION RULES**

- **Silent execution**: NEVER use `tee` when running build or test commands. Redirect all output to log files (`> "$LOG_FILE" 2>&1`). Only inspect logs when command returns non-zero exit code.
- **Exit code checking**: Always capture and check exit codes to resolve build and test success/failure. Zero = success, non-zero = failure.
- **No assumptions**: Never assume errors are "pre-existing" or skip investigating them. All errors must be analyzed unless user explicitly stops the workflow.
- **No Git Commits:** DO NOT commit changes as part of this workflow. Users will do that separately.
