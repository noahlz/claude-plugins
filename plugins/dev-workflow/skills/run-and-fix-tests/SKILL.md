---
name: run-and-fix-tests
description: Build the project, run tests and systematically fix any failures. Activate when user says: "run tests", "run the tests", "test this/it/the changes", "execute tests", "build and test", "fix tests", "make sure tests pass", "verify with tests", "check if tests work/pass", "verify the fix/changes", "see if this/it works", "check that/if it works", OR when user mentions "failing/failed tests", "test failures", "CI failing", "GitHub Actions failing", "tests not passing", OR after making code changes to verify they work, OR when tests are the logical next development step.
---

This skill streamlines running and fixing unit tests in a project. It:
- resolves the project build/test commands from project-specific configuration, generating it for future use (with user input), if needed.
- strives for minimal token / context usage by redirecting build/test output to files

The skill delegates to sub-agents when there are a large number (10+) of test failures or build errors:
  - 'build-fixer' to fix compilation errors
  - 'test-fixer' to fix test failures

Activate this skill proactively after making code changes to verify they work (suggest first: "Should I run the test suite to verify these changes?").

Also activate this skill when the user requests testing using phrases like:
- "run tests"
- "test the changes"
- "build and test"
- "fix failing tests"

---

**âš ï¸ CRITICAL: HOW TO EXECUTE BASH CODE IN THIS SKILL**

When you see inline bash code blocks (```bash), you MUST:
- Execute them using the Bash tool
- NEVER narrate execution without actually running the command
- NEVER fabricate outputs

When instructed to "Execute from [file.md]" or "Execute instructions from [file.md]":
1. Read the markdown file using Read tool
2. Find the relevant bash code blocks
3. Execute those code blocks using Bash tool
4. Handle results as described in the file

**Failure to execute commands results in workflow corruption and invalid test runs.**

---

**âš ï¸  CRITICAL BUILD/TEST EXECUTION RULES**

- **Silent execution**: NEVER use `tee` when running builds or test commands. Redirect all output to log files (`> "$LOG_FILE" 2>&1`). Only inspect logs when command returns non-zero exit code.
- **Exit code checking**: Always capture and check exit codes to resolve build and test success/failure. Zero = success, non-zero = failure.
- **No assumptions**: Never assume errors are "pre-existing" or skip investigating them. All errors must be analyzed unless user explicitly stops the workflow.
- **No Git Commits:** DO NOT commit changes as part of this workflow. Users will do that separately.

---

## 0. Prerequisites

**SKILL_NAME**: run-and-fix-tests

**CLAUDE_PLUGIN_ROOT**: !`if [ -x "$HOME/.claude/resolve_plugin_root.sh" ]; then $HOME/.claude/resolve_plugin_root.sh "dev-workflow@noahlz.github.io"; elif [ -x "./.claude/resolve_plugin_root.sh" ]; then ./.claude/resolve_plugin_root.sh "dev-workflow@noahlz.github.io"; else echo "âš ï¸ Run dev-workflow:setup to install resolver"; fi`

---

If you see "âš ï¸ Run dev-workflow:setup" above, the resolver script is missing. Stop and run the setup skill.

**âš ï¸ CRITICAL**: Use the `CLAUDE_PLUGIN_ROOT` value shown above in subsequent commands in this skill. Either interpolate the literal value or prefix each bash command with the value i.e. `CLAUDE_PLUGIN_ROOT=(literal value) (bash command)`

## 1. Detect Build Configuration

**Build configuration status**: !`[ -f "./.claude/settings.plugins.run-and-fix-tests.json" ] && echo "âœ“ Config found" || echo "âš ï¸ Config setup required"`

---

**Result handling:**  
âœ“ If you see "âœ“ Config found" above â†’ Config exists, proceed to Section 2  
âœ— If you see "âš ï¸ Config setup required" above â†’ Config missing, proceed to Section 1a  

## 1a. Setup Build Configuration (First Run Only)

Execute ONLY if Section 1 shows "âš ï¸ Config setup required".

â†’ Execute setup instructions from `./references/setup-config.md`

**Result handling:**  
âœ“ Exit 0 â†’ Config created, proceed to Section 2  
âœ— Exit 1 â†’ Display error: "No build tools found. Create `.claude/settings.plugins.run-and-fix-tests.json` manually"  
âš ï¸ Exit 2 â†’ Display warning: "Placeholder config created. Edit `.claude/settings.plugins.run-and-fix-tests.json` before proceeding"  

## 2. Load Configuration

â†’ Execute load-config script to output configuration:
```bash
node ${CLAUDE_PLUGIN_ROOT}/skills/run-and-fix-tests/scripts/load-config.js "${CLAUDE_PLUGIN_ROOT}"
```

**âš ï¸ CRITICAL - Capture Output Values**

The script outputs key=value pairs. Example:
```
TEST_CMD=npm test
TEST_RESULTS_PATH=dist/test-results.tap
TEST_ERROR_PATTERN=(not ok|Bail out!)
TEST_SINGLE_CMD=npm test -- {testFile}
TEST_SINGLE_RESULTS_PATH=dist/test-single-results.tap
TEST_SINGLE_ERROR_PATTERN=(not ok|Bail out!)
TEST_LOG=dist/test.log
OUT_DIR=dist
BUILD_COUNT=0
SKIP_BUILD=true
```

**Remember these values** You will use the literal values (not shell variables like `$TEST_CMD`) in subsequent bash commands.

âœ— Script fails â†’ Display error and stop  
âœ“ Script succeeds â†’ Values captured, proceed to Section 3   

## 3. Build Project

â†’ Check the SKIP_BUILD value captured from Section 2 (literal value, not shell variable)

**If SKIP_BUILD=true:**  
â†’ Display: "Build step skipped (build command identical to test command)"  
â†’ Proceed directly to step 4 (Run Tests)  

**If SKIP_BUILD=false:**

â†’ Use the BUILD_COUNT value from Section 2. If BUILD_COUNT=0, no build steps exist, proceed to step 4.

â†’ For each build index from 0 to (BUILD_COUNT - 1), use the captured literal values:
  - BUILD_0_CMD, BUILD_0_LOG, BUILD_0_WORKING_DIR, BUILD_0_ERROR_PATTERN (if BUILD_COUNT >= 1)
  - BUILD_1_CMD, BUILD_1_LOG, BUILD_1_WORKING_DIR, BUILD_1_ERROR_PATTERN (if BUILD_COUNT >= 2)
  - etc.

â†’ For each build:
  - Change to working directory using the captured BUILD_i_WORKING_DIR value
  - Execute the build command using the captured BUILD_i_CMD value, redirect output to captured BUILD_i_LOG
  - Check exit code: if non-zero, record failure and continue to next build

â†’ If any builds fail:
  - Collect error logs from all failed builds
  - Use the BUILD_i_ERROR_PATTERN regex to parse errors from each log
  - Proceed to step 3a with aggregated error list

âœ“ All builds succeed â†’ Proceed to step 4 (Run Tests)

## 3a. Extract Build Errors

â†’ Extract build errors (see ./references/build-procedures.md)

â†’ Use AskUserQuestion: "Build failed with [N] compilation errors. Fix them?"
  - "Yes" â†’ Proceed to step 3b
  - "No" â†’ Stop

## 3b. Delegate to Build-Fixer Agent

â†’ Delegate to build-fixer (see ./references/agent-delegation.md)
  - Provide error list from step 3a
  - Provide BUILD_FIXER_ENV_VARS (see ./references/agent-delegation.md)

âœ“ Agent completes â†’ Proceed to step 3c

## 3c. Rebuild After Fixes

â†’ Rebuild and verify (see ./references/build-procedures.md)  
âœ“ Build succeeds â†’ Proceed to Section 4 (Run Tests)  
âœ— Build fails â†’ Return to Section 3a (more errors)  

## 4. Run Tests

â†’ Use the literal values captured from Section 2 (not shell variables):

**Single test mode** (if running a specific test):
  - Use TEST_SINGLE_CMD value with {testFile} replaced
  - Redirect output to TEST_SINGLE_RESULTS_PATH value

**All tests mode** (normal case):
  - Use TEST_CMD value (captured literal, e.g., "npm test")
  - Redirect output to TEST_RESULTS_PATH value (e.g., "dist/test-results.tap")
  - Optionally capture human-readable output to TEST_LOG value (e.g., "dist/test.log")

Example bash command using literal values:  
```bash
npm test > dist/test-results.tap 2>&1
```

â†’ Execute test command and capture exit code  
âœ“ Exit 0 â†’ All tests pass, proceed to step 8 (Completion)  
âœ— Exit non-zero â†’ Tests failed, proceed to step 5 (Extract Test Errors)  

## 5. Extract Test Errors

â†’ Parse test results file using the captured literal values from Section 2:
  - Read the file at TEST_RESULTS_PATH (e.g., "dist/test-results.tap")
  - Extract failures using TEST_ERROR_PATTERN regex (e.g., "(not ok|Bail out!)")

â†’ For detailed extraction procedure, see ./references/build-procedures.md

âœ“ 0 failures detected â†’ Proceed to step 8 (Completion)  
âœ— 1-30 failures â†’ Display error summary, proceed to step 6  
âœ— 30+ failures â†’ Display count, proceed to step 6  

## 6. Ask to Fix Tests

â†’ Check failure count from step 5:

**If 30+ failures:**  
âš ï¸ Display: "30+ tests failed. This is too many for efficient fixing in one chat."  
â†’ Use AskUserQuestion:  
  - "Attempt to fix 30+ tests?" (not recommended)  
  - "No, I'll stop and create a plan"  

â†’ If "No" â†’ Stop (user exits to create plan)  
â†’ If "Yes" â†’ Continue to step 7  

**If 1-29 failures:**  
â†’ Use AskUserQuestion:  
  - "Start fixing tests?" (recommended)
  - "No, I'll fix manually"

â†’ If "Yes" â†’ Continue to step 7  
â†’ If "No" â†’ Stop  

## 7. Delegate to Test-Fixer Agent

â†’ Delegate to the `test-fixer` agent to fix failing tests one-by-one

â†’ Store agent ID for potential resumption: `TEST_FIXER_AGENT_ID=[agent_id]`

â†’ Provide agent with context in natural language:
  - Failed test list: [bulleted list with test names and error excerpts from step 5]
  - Example failed test entry: "TestLoginFlow (test/auth.test.js) - Expected 'logged in', got undefined"

â†’ Provide TEST_FIXER_ENV_VARS (see ./references/agent-delegation.md)

â†’ Agent fixes the tests per its instructions and context provided

âœ“ Agent completes without delegation â†’ Proceed to step 7d  
ğŸ”„ Agent exits with COMPILATION_ERROR delegation â†’ Proceed to step 7b  

## 7b. Handle Compilation Error Delegation

â†’ Detect delegation signal in test-fixer's final message:  
Look for: "ğŸ”„ DELEGATION_REQUIRED: COMPILATION_ERROR"

â†’ Extract build errors (see ./references/build-procedures.md)

â†’ Use AskUserQuestion:
  - "Test fix introduced compilation errors. Fix them with build-fixer?"
  - "Yes" â†’ Continue to step 7c
  - "No" â†’ Proceed to step 7d

## 7c. Invoke Build-Fixer and Resume Test-Fixer

â†’ Delegate to build-fixer (see ./references/agent-delegation.md)

â†’ Rebuild and verify (see ./references/build-procedures.md)
  - If build fails: Return to step 7b (more compilation errors)
  - If build succeeds: Continue to resume test-fixer

â†’ Resume test-fixer (see ./references/agent-delegation.md)

âœ“ Test-fixer completes â†’ Proceed to step 7d  
ğŸ”„ Test-fixer delegates again â†’ Loop back to step 7b (compilation errors reintroduced)  

## 7d. Ask User to Re-run Tests

â†’ Use AskUserQuestion:
  - "Re-run all tests to verify fixes?"
  - "No, stop for now"

âœ“ User confirms â†’ Proceed to step 4 (Run Tests)  
âœ— User declines â†’ Proceed to step 8  

## 8. Completion

â†’ Check if all originally-failing tests were fixed:
  - If yes â†’ Display: "âœ… All tests fixed and passed!"
  - If no â†’ Display: "âš ï¸ Workflow incomplete. Some tests remain unfixed."

â†’ Show summary:
  - Tests fixed in this session
  - Tests skipped/remaining
  - Root causes addressed

â†’ Clear todo list with TodoWrite (empty)  
â†’ Exit  
