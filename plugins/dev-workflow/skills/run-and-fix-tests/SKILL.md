---
name: run-and-fix-tests
description: Build the project, run tests and systematically fix any failures. Activate when user says: "run tests", "run the tests", "test this/it/the changes", "execute tests", "build and test", "fix tests", "make sure tests pass", "verify with tests", "check if tests work/pass", "verify the fix/changes", "see if this/it works", "check that/if it works", OR when user mentions "failing/failed tests", "test failures", "CI failing", "GitHub Actions failing", "tests not passing", OR after making code changes to verify they work, OR when tests are the logical next development step.
---

This skill streamlines running and fixing unit tests in a project. It:
- resolves the project build/test commands from project-specific configuration, generating it for future use (with user input), if needed.
- strives for minimal token / context usage by redirecting build/test output to files

The skill delegates to sub-agents when there are a large number (10+) of test failures or build errors:
  - 'build-fixer' to fix compilation errors
  - 'test-fixer' to fix test failures

Activate this skill proactively after making code changes to verify they work (suggest first: "Should I run the test suite to verify these changes?").

Also activate this skill when the user requests testing using phrases like:
- "run tests"
- "test the changes"
- "build and test"
- "fix failing tests"

---

**âš ï¸ CRITICAL: HOW TO EXECUTE BASH CODE IN THIS SKILL**

When you see inline bash code blocks (```bash), you MUST:
- Execute them using the Bash tool
- NEVER narrate execution without actually running the command
- NEVER fabricate outputs

When instructed to "Execute from [file.md]" or "Execute instructions from [file.md]":
1. Read the markdown file using Read tool
2. Find the relevant bash code blocks
3. Execute those code blocks using Bash tool
4. Handle results as described in the file

**Failure to execute commands results in workflow corruption and invalid test runs.**

---

**âš ï¸  CRITICAL BUILD/TEST EXECUTION RULES**

- **Silent execution**: NEVER use `tee` when running builds or test commands. Redirect all output to log files (`> "$LOG_FILE" 2>&1`). Only inspect logs when command returns non-zero exit code.
- **Exit code checking**: Always capture and check exit codes to resolve build and test success/failure. Zero = success, non-zero = failure.
- **No assumptions**: Never assume errors are "pre-existing" or skip investigating them. All errors must be analyzed unless user explicitly stops the workflow.
- **No Git Commits:** DO NOT commit changes as part of this workflow. Users will do that separately.

---

## 0. Prerequisites

**Step description**: "Checking prerequisites"

â†’ Execute prerequisite check using Bash tool:
```bash
# 1. Check for resolver script (look in ./.claude first, then $HOME/.claude)
RESOLVER=""
if [ -x "$HOME/.claude/resolve_plugin_root.sh" ]; then
  RESOLVER="$HOME/.claude/resolve_plugin_root.sh"
elif [ -x "./.claude/resolve_plugin_root.sh" ]; then
  RESOLVER="./.claude/resolve_plugin_root.sh"
else
  echo "âš ï¸ Missing plugin resolver script"
  echo ""
  echo "Run the setup skill to create it:"
  echo ""
  echo "  dev-workflow:setup"
  echo ""
  exit 1
fi

# 2. Resolve plugin root
CLAUDE_PLUGIN_ROOT="$($RESOLVER "dev-workflow@noahlz.github.io")" || {
  echo "âš ï¸ Failed to resolve plugin root?!?"
  exit 1
}

# 3. Output for LLM to capture
echo "CLAUDE_PLUGIN_ROOT=$CLAUDE_PLUGIN_ROOT"
echo "SKILL_NAME=write-git-comit
```

**Result handling:**  
âœ“ Exit 0 â†’ Prerequisites met, **LLM captures CLAUDE_PLUGIN_ROOT from output**, proceed to section 1  
âœ— Exit 1 â†’ Prerequisites missing, display error and **STOP** (no fallback)  

**âš ï¸ CRITICAL**: Use the `CLAUDE_PLUGIN_ROOT` value output in subsequent commands in this skill. Either interpolate the literal value or prefix each bash command with the value i.e. `CLAUDE_PLUGIN_ROOT=(literal value) (bash command)`

## 1. Detect Build Configuration

**Step description**: "Checking build configuration"

â†’ Fast path check (config exists):
```bash
if [ -f "./.claude/settings.plugins.run-and-fix-tests.json" ]; then
  echo "âœ“ Config found"
else
  echo "âš ï¸ Config setup required"
  exit 1
fi
```

**Result handling:**  
âœ“ Exit 0 â†’ Config exists, proceed to Section 2  
âœ— Exit 1 â†’ Config missing, proceed to Section 1a  

## 1a. Setup Build Configuration (First Run Only)

Execute ONLY if section 1 returned exit 1.

â†’ Execute setup instructions from `${CLAUDE_PLUGIN_ROOT}/skills/run-and-fix-tests/setup-config.md`

**Result handling:**  
âœ“ Exit 0 â†’ Config created, proceed to Section 2  
âœ— Exit 1 â†’ Display error: "No build tools found. Create `.claude/settings.plugins.run-and-fix-tests.json` manually"  
âš ï¸ Exit 2 â†’ Display warning: "Placeholder config created. Edit `.claude/settings.plugins.run-and-fix-tests.json` before proceeding"  

## 2. Load Configuration

â†’ Execute load-config script to output configuration as eval-able statements:
```bash
eval "$(node ${CLAUDE_PLUGIN_ROOT}/skills/run-and-fix-tests/scripts/load-config.js "${CLAUDE_PLUGIN_ROOT}")"
```
âœ— Script fails â†’ Display error and stop  
âœ“ Script succeeds â†’ Environment variables set:
  - BUILD_COUNT, BUILD_{i}_CMD, BUILD_{i}_LOG, BUILD_{i}_ERROR_PATTERN, BUILD_{i}_WORKING_DIR (for each build step)
  - TEST_CMD, TEST_RESULTS_PATH, TEST_ERROR_PATTERN
  - TEST_SINGLE_CMD, TEST_SINGLE_RESULTS_PATH, TEST_SINGLE_ERROR_PATTERN
  - TEST_LOG (optional, for human-readable logs)
  - OUT_DIR (build output directory, e.g., dist/, build/, target/)

â†’ Check command argument: `TEST_FILE="$1"`
â†’ Determine mode:
  - `$TEST_FILE` not empty â†’ Single test mode
  - `$TEST_FILE` empty â†’ All tests mode

â†’ Store initial working directory: `INITIAL_PWD=$(pwd)`

â†’ Determine build count: `BUILD_COUNT=$BUILD_COUNT` (number of indexed build steps)

## 3. Build Project

â†’ Check if build should be skipped: `$SKIP_BUILD`

**Skip Build (SKIP_BUILD=true):**  
â†’ Display: "Build step skipped (build command identical to test command)"  
â†’ Proceed directly to step 4 (Run Tests)  

**Run Build (SKIP_BUILD=false):**
â†’ Create output directory: `mkdir -p "$OUT_DIR"`  
â†’ Iterate through all builds by index:  
  â†’ For each index i from 0 to (BUILD_COUNT - 1):  
  - Extract variables: `BUILD_${i}_CMD`, `BUILD_${i}_LOG`, `BUILD_${i}_WORKING_DIR`, `BUILD_${i}_ERROR_PATTERN`
  - Change to working directory: `cd "${BUILD_${i}_WORKING_DIR}"`
  - Execute build command: `${BUILD_${i}_CMD} > "${BUILD_${i}_LOG}" 2>&1`
  - Check exit code:
    - Exit 0: continue to next build
    - Exit non-zero: record failure, continue collecting all errors

â†’ When builds fail:
  - Collect error logs from all failed builds
  - Parse each log using its specific BUILD_${i}_ERROR_PATTERN
  - Return to INITIAL_PWD, proceed to step 3a with aggregated error list

âœ“ All builds succeed â†’ Return to INITIAL_PWD, proceed to step 4 (Run Tests)

## 3a. Extract Build Errors

â†’ Extract build errors (see ${CLAUDE_PLUGIN_ROOT}/skills/run-and-fix-tests/build-procedures.md)

â†’ Use AskUserQuestion: "Build failed with [N] compilation errors. Fix them?"
  - "Yes" â†’ Proceed to step 3b
  - "No" â†’ Stop

## 3b. Delegate to Build-Fixer Agent

â†’ Delegate to build-fixer (see ${CLAUDE_PLUGIN_ROOT}/run-and-fix-tests/agent-delegation.md)
  - Provide error list from step 3a
  - Provide BUILD_FIXER_ENV_VARS (see ${CLAUDE_PLUGIN_ROOT}/skills/run-and-fix-tests/agent-delegation.md)

âœ“ Agent completes â†’ Proceed to step 3c

## 3c. Rebuild After Fixes

â†’ Rebuild and verify (see ${CLAUDE_PLUGIN_ROOT}/skills/run-and-fix-tests/build-procedures.md)  
âœ“ Build succeeds â†’ Proceed to Section 4 (Run Tests)  
âœ— Build fails â†’ Return to Section 3a (more errors)  

## 4. Run Tests

â†’ Determine test command based on mode:
  - Single test mode: use `$TEST_SINGLE_CMD` with {testFile} replaced, results to `$TEST_SINGLE_RESULTS_PATH`
  - All tests mode: use `$TEST_CMD`, results to `$TEST_RESULTS_PATH`

â†’ Change to test working directory (if different from build dir)
â†’ Execute test command with output redirected to results file (tool-specific)
â†’ If `$TEST_LOG` is set, also capture human-readable output to that file (optional)  
âœ“ Exit 0 â†’ Return to INITIAL_PWD, all tests pass, proceed to step 8 (Completion)  
âœ— Exit non-zero â†’ Return to INITIAL_PWD, tests failed, proceed to step 5 (Extract Test Errors)  

## 5. Extract Test Errors

â†’ Extract test errors (see ${CLAUDE_PLUGIN_ROOT}/skills/run-and-fix-tests/build-procedures.md)

âœ“ 0 failures detected â†’ Proceed to step 8 (Completion)  
âœ— 1-30 failures â†’ Display error summary, proceed to step 6  
âœ— 30+ failures â†’ Display count, proceed to step 6  

## 6. Ask to Fix Tests

â†’ Check failure count from step 5:

**If 30+ failures:**  
âš ï¸ Display: "30+ tests failed. This is too many for efficient fixing in one chat."  
â†’ Use AskUserQuestion:  
  - "Attempt to fix 30+ tests?" (not recommended)  
  - "No, I'll stop and create a plan"  

â†’ If "No" â†’ Stop (user exits to create plan)  
â†’ If "Yes" â†’ Continue to step 7  

**If 1-29 failures:**  
â†’ Use AskUserQuestion:  
  - "Start fixing tests?" (recommended)
  - "No, I'll fix manually"

â†’ If "Yes" â†’ Continue to step 7  
â†’ If "No" â†’ Stop  

## 7. Delegate to Test-Fixer Agent

â†’ Delegate to the `test-fixer` agent to fix failing tests one-by-one

â†’ Store agent ID for potential resumption: `TEST_FIXER_AGENT_ID=[agent_id]`

â†’ Provide agent with context in natural language:
  - Failed test list: [bulleted list with test names and error excerpts from step 5]
  - Example failed test entry: "TestLoginFlow (test/auth.test.js) - Expected 'logged in', got undefined"

â†’ Provide TEST_FIXER_ENV_VARS (see ${CLAUDE_PLUGIN_ROOT}/skills/run-and-fix-tests/agent-delegation.md)

â†’ Agent fixes the tests per its instructions and context provided

âœ“ Agent completes without delegation â†’ Proceed to step 7d  
ğŸ”„ Agent exits with COMPILATION_ERROR delegation â†’ Proceed to step 7b  

## 7b. Handle Compilation Error Delegation

â†’ Detect delegation signal in test-fixer's final message:  
Look for: "ğŸ”„ DELEGATION_REQUIRED: COMPILATION_ERROR"

â†’ Extract build errors (see ${CLAUDE_PLUGIN_ROOT}/skills/run-and-fix-tests/build-procedures.md)

â†’ Use AskUserQuestion:
  - "Test fix introduced compilation errors. Fix them with build-fixer?"
  - "Yes" â†’ Continue to step 7c
  - "No" â†’ Proceed to step 7d

## 7c. Invoke Build-Fixer and Resume Test-Fixer

â†’ Delegate to build-fixer (see ${CLAUDE_PLUGIN_ROOT}/skills/run-and-fix-tests/agent-delegation.md)

â†’ Rebuild and verify (see ${CLAUDE_PLUGIN_ROOT}/skills/run-and-fix-tests/build-procedures.md)
  - If build fails: Return to step 7b (more compilation errors)
  - If build succeeds: Continue to resume test-fixer

â†’ Resume test-fixer (see ${CLAUDE_PLUGIN_ROOT}/skills/run-and-fix-tests/agent-delegation.md)

âœ“ Test-fixer completes â†’ Proceed to step 7d  
ğŸ”„ Test-fixer delegates again â†’ Loop back to step 7b (compilation errors reintroduced)  

## 7d. Ask User to Re-run Tests

â†’ Use AskUserQuestion:
  - "Re-run all tests to verify fixes?"
  - "No, stop for now"

âœ“ User confirms â†’ Proceed to step 4 (Run Tests)  
âœ— User declines â†’ Proceed to step 8  

## 8. Completion

â†’ Check if all originally-failing tests were fixed:
  - If yes â†’ Display: "âœ… All tests fixed and passed!"
  - If no â†’ Display: "âš ï¸ Workflow incomplete. Some tests remain unfixed."

â†’ Show summary:
  - Tests fixed in this session
  - Tests skipped/remaining
  - Root causes addressed

â†’ Clear todo list with TodoWrite (empty)  
â†’ Exit  
