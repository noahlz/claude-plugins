---
name: run-and-fix-tests
description: Build the project, run tests and systematically fix any failures. Activate when user says: "run tests", "run the tests", "test this/it/the changes", "execute tests", "build and test", "fix tests", "make sure tests pass", "verify with tests", "check if tests work/pass", "verify the fix/changes", "see if this/it works", "check that/if it works", OR when user mentions "failing/failed tests", "test failures", "CI failing", "GitHub Actions failing", "tests not passing", OR after making code changes to verify they work, OR when tests are the logical next development step.
---

This skill streamlines running and fixing unit tests in a project. It:
- resolves the project build/test commands from project-specific configuration, generating it for future use (with user input), if needed.
- strives for minimal token / context usage by redirecting build/test output to files

The skill delegates to sub-agents when appropriate:
  - 'build-fixer' to fix compilation errors
  - 'test-fixer' to fix test failures

Activate this skill proactively after making code changes to verify they work (suggest first: "Should I run the test suite to verify these changes?").

Also activate this skill when the user requests testing using phrases like:
- "run tests"
- "test the changes"
- "build and test"
- "fix failing tests"

---

**‚ö†Ô∏è CRITICAL: HOW TO EXECUTE BASH CODE IN THIS SKILL**

When you see inline bash code blocks (```bash), you MUST:
- Execute them using the Bash tool
- NEVER narrate execution without actually running the command
- NEVER fabricate outputs

When instructed to "Execute from [file.md]" or "Execute instructions from [file.md]":
1. Read the markdown file using Read tool
2. Find the relevant bash code blocks
3. Execute those code blocks using Bash tool
4. Handle results as described in the file

**Failure to execute commands results in workflow corruption and invalid test runs.**

---

## 0. Prerequisites

**Step description**: "Checking prerequisites"

‚Üí Execute prerequisite check using Bash tool:
```bash
SKILL_NAME="run-and-fix-tests"

# 1. Check Node.js version
if ! command -v node >/dev/null 2>&1; then
  echo "‚ö†Ô∏è Node.js 22+ required"
  echo "Install from https://nodejs.org/"
  exit 1
fi
NODE_MAJOR=$(node -v | cut -d'.' -f1 | sed 's/v//')
if [ "$NODE_MAJOR" -lt 22 ]; then
  echo "‚ö†Ô∏è Node.js $(node -v) found, but 22+ required"
  echo "Install from https://nodejs.org/"
  exit 1
fi

# 2. Check for resolver script (look in ./.claude first, then $HOME/.claude)
RESOLVER=""
if [ -x "./.claude/resolve_plugin_root.sh" ]; then
  RESOLVER="./.claude/resolve_plugin_root.sh"
elif [ -x "$HOME/.claude/resolve_plugin_root.sh" ]; then
  RESOLVER="$HOME/.claude/resolve_plugin_root.sh"
else
  echo "‚ö†Ô∏è Missing plugin resolver script"
  echo ""
  echo "Run the setup skill to create it:"
  echo "  Use the dev-workflow:setup skill"
  echo ""
  exit 1
fi

# 3. Resolve plugin root
CLAUDE_PLUGIN_ROOT="$($RESOLVER "dev-workflow@noahlz.github.io")" || {
  echo "‚ö†Ô∏è Failed to resolve plugin root"
  echo "Try running the setup skill again:"
  echo "  Use the dev-workflow:setup skill"
  exit 1
}

# 4. Output for LLM to capture
echo "CLAUDE_PLUGIN_ROOT=$CLAUDE_PLUGIN_ROOT"
echo "‚úì Ready (Node $(node -v))"
```

‚ö†Ô∏è CHECKPOINT: Verify you actually executed Bash tool above
- If you narrated without running Bash: STOP and run the commands now
- Check exit code to determine next step

**Result handling:**
‚úì Exit 0 ‚Üí Prerequisites met, **LLM captures CLAUDE_PLUGIN_ROOT from output**, proceed to section 1
‚úó Exit 1 ‚Üí Prerequisites missing, display error and **STOP** (no fallback)

**‚ö†Ô∏è CRITICAL**: After Section 0 succeeds, you MUST capture the `CLAUDE_PLUGIN_ROOT=<path>` value from the bash output above. Use this captured value in all subsequent bash commands that reference plugin scripts (don't use `${CLAUDE_PLUGIN_ROOT}` in bash, substitute the actual path directly). This is necessary because environment variables don't persist between separate Bash tool invocations.

## 1. Detect Build Configuration

**Step description**: "Checking build configuration"

‚Üí Fast path check (config exists):
```bash
if [ -f "./.claude/settings.plugins.run-and-fix-tests.json" ]; then
  echo "‚úì Config found"
else
  echo "‚ö†Ô∏è Config setup required"
  exit 1
fi
```

**Result handling:**
‚úì Exit 0 ‚Üí Config exists, proceed to Section 2
‚úó Exit 1 ‚Üí Config missing, proceed to Section 1a

## 1a. Setup Build Configuration (First Run Only)

Execute ONLY if section 1 returned exit 1.

‚Üí Execute setup instructions from `${CLAUDE_PLUGIN_ROOT}/skills/run-and-fix-tests/setup-config.md`

**Result handling:**
‚úì Exit 0 ‚Üí Config created, proceed to Section 2
‚úó Exit 1 ‚Üí Display error: "No build tools found. Create `.claude/settings.plugins.run-and-fix-tests.json` manually"
‚ö†Ô∏è Exit 2 ‚Üí Display warning: "Placeholder config created. Edit `.claude/settings.plugins.run-and-fix-tests.json` before proceeding"

## 2. Load Configuration

‚Üí Execute load-config script to output configuration as eval-able statements:
```bash
eval "$(node ${CLAUDE_PLUGIN_ROOT}/skills/run-and-fix-tests/scripts/load-config.js "${CLAUDE_PLUGIN_ROOT}")"
```
‚úó Script fails ‚Üí Display error and stop  
‚úì Script succeeds ‚Üí Environment variables set:
  - BUILD_COUNT, BUILD_{i}_CMD, BUILD_{i}_LOG, BUILD_{i}_ERROR_PATTERN, BUILD_{i}_WORKING_DIR (for each build step)
  - TEST_CMD, TEST_RESULTS_PATH, TEST_ERROR_PATTERN
  - TEST_SINGLE_CMD, TEST_SINGLE_RESULTS_PATH, TEST_SINGLE_ERROR_PATTERN
  - TEST_LOG (optional, for human-readable logs)
  - OUT_DIR (build output directory, e.g., dist/, build/, target/)

‚Üí Check command argument: `TEST_FILE="$1"`
‚Üí Determine mode:
  - `$TEST_FILE` not empty ‚Üí Single test mode
  - `$TEST_FILE` empty ‚Üí All tests mode

‚Üí Store initial working directory: `INITIAL_PWD=$(pwd)`

‚Üí Determine build count: `BUILD_COUNT=$BUILD_COUNT` (number of indexed build steps)

## 3. Build Project

‚Üí Check if build should be skipped: `$SKIP_BUILD`

**Skip Build (SKIP_BUILD=true):**
‚Üí Display: "Build step skipped (build command identical to test command)"
‚Üí Proceed directly to step 4 (Run Tests)

**Run Build (SKIP_BUILD=false):**
‚Üí Create output directory: `mkdir -p "$OUT_DIR"`
‚Üí Iterate through all builds by index:
  ‚Üí For each index i from 0 to (BUILD_COUNT - 1):
    - Extract variables: BUILD_${i}_CMD, BUILD_${i}_LOG, BUILD_${i}_WORKING_DIR, BUILD_${i}_ERROR_PATTERN
    - Change to working directory: `cd "${BUILD_${i}_WORKING_DIR}"`
    - Execute build command: `${BUILD_${i}_CMD} > "${BUILD_${i}_LOG}" 2>&1`
    - Check exit code:
      - Exit 0: continue to next build
      - Exit non-zero: record failure, continue collecting all errors

‚Üí When builds fail:
  - Collect error logs from all failed builds
  - Parse each log using its specific BUILD_${i}_ERROR_PATTERN
  - Return to INITIAL_PWD, proceed to step 3a with aggregated error list

‚úì All builds succeed ‚Üí Return to INITIAL_PWD, proceed to step 4 (Run Tests)

## 3a. Extract Build Errors

‚Üí Extract build errors (see ${CLAUDE_PLUGIN_ROOT}/skills/run-and-fix-tests/build-procedures.md)

‚Üí Use AskUserQuestion: "Build failed with [N] compilation errors. Fix them?"
  - "Yes" ‚Üí Proceed to step 3b
  - "No" ‚Üí Stop

## 3b. Delegate to Build-Fixer Agent

‚Üí Delegate to build-fixer (see ${CLAUDE_PLUGIN_ROOT}/run-and-fix-tests/agent-delegation.md)
  - Provide error list from step 3a
  - Provide BUILD_FIXER_ENV_VARS (see ${CLAUDE_PLUGIN_ROOT}/skills/run-and-fix-tests/agent-delegation.md#build-fixer-env-vars)

‚úì Agent completes ‚Üí Proceed to step 3c

## 3c. Rebuild After Fixes

‚Üí Rebuild and verify (see ${CLAUDE_PLUGIN_ROOT}/skills/run-and-fix-tests/build-procedures.md)
‚úì Build succeeds ‚Üí Proceed to Section 4 (Run Tests)
‚úó Build fails ‚Üí Return to Section 3a (more errors)

## 4. Run Tests

‚Üí Determine test command based on mode:
  - Single test mode: use `$TEST_SINGLE_CMD` with {testFile} replaced, results to `$TEST_SINGLE_RESULTS_PATH`
  - All tests mode: use `$TEST_CMD`, results to `$TEST_RESULTS_PATH`

‚Üí Change to test working directory (if different from build dir)
‚Üí Execute test command with output redirected to results file (tool-specific)
‚Üí If `$TEST_LOG` is set, also capture human-readable output to that file (optional)  
‚úì Exit 0 ‚Üí Return to INITIAL_PWD, all tests pass, proceed to step 8 (Completion)  
‚úó Exit non-zero ‚Üí Return to INITIAL_PWD, tests failed, proceed to step 5 (Extract Test Errors)  

## 5. Extract Test Errors

‚Üí Extract test errors (see ${CLAUDE_PLUGIN_ROOT}/skills/run-and-fix-tests/build-procedures.md)

‚úì 0 failures detected ‚Üí Proceed to step 8 (Completion)
‚úó 1-30 failures ‚Üí Display error summary, proceed to step 6
‚úó 30+ failures ‚Üí Display count, proceed to step 6

## 6. Ask to Fix Tests

‚Üí Check failure count from step 5:

**If 30+ failures:**  
‚ö†Ô∏è Display: "30+ tests failed. This is too many for efficient fixing in one chat."  
‚Üí Use AskUserQuestion:  
  - "Attempt to fix 30+ tests?" (not recommended)  
  - "No, I'll stop and create a plan"  
‚Üí If "No" ‚Üí Stop (user exits to create plan)  
‚Üí If "Yes" ‚Üí Continue to step 7  

**If 1-29 failures:**  
‚Üí Use AskUserQuestion:  
  - "Start fixing tests?" (recommended)
  - "No, I'll fix manually"
‚Üí If "Yes" ‚Üí Continue to step 7  
‚Üí If "No" ‚Üí Stop  

## 7. Delegate to Test-Fixer Agent

‚Üí Delegate to the `test-fixer` agent to fix failing tests one-by-one

‚Üí Store agent ID for potential resumption: `TEST_FIXER_AGENT_ID=[agent_id]`

‚Üí Provide agent with context in natural language:
  - Failed test list: [bulleted list with test names and error excerpts from step 5]
  - Example failed test entry: "TestLoginFlow (test/auth.test.js) - Expected 'logged in', got undefined"

‚Üí Provide TEST_FIXER_ENV_VARS (see ${CLAUDE_PLUGIN_ROOT}/skills/run-and-fix-tests/agent-delegation.md#test-fixer-env-vars)

‚Üí Agent fixes the tests per its instructions and context provided

‚úì Agent completes without delegation ‚Üí Proceed to step 7d  
üîÑ Agent exits with COMPILATION_ERROR delegation ‚Üí Proceed to step 7b  

## 7b. Handle Compilation Error Delegation

‚Üí Detect delegation signal in test-fixer's final message:  
Look for: "üîÑ DELEGATION_REQUIRED: COMPILATION_ERROR"

‚Üí Extract build errors (see ${CLAUDE_PLUGIN_ROOT}/skills/run-and-fix-tests/build-procedures.md)

‚Üí Use AskUserQuestion:
  - "Test fix introduced compilation errors. Fix them with build-fixer?"
  - "Yes" ‚Üí Continue to step 7c
  - "No" ‚Üí Proceed to step 7d

## 7c. Invoke Build-Fixer and Resume Test-Fixer

‚Üí Delegate to build-fixer (see ${CLAUDE_PLUGIN_ROOT}/skills/run-and-fix-tests/agent-delegation.md#delegate-to-build-fixer)

‚Üí Rebuild and verify (see ${CLAUDE_PLUGIN_ROOT}/skills/run-and-fix-tests/build-procedures.md)
  - If build fails: Return to step 7b (more compilation errors)
  - If build succeeds: Continue to resume test-fixer

‚Üí Resume test-fixer (see ${CLAUDE_PLUGIN_ROOT}/skills/run-and-fix-tests/agent-delegation.md)

‚úì Test-fixer completes ‚Üí Proceed to step 7d
üîÑ Test-fixer delegates again ‚Üí Loop back to step 7b (compilation errors reintroduced)  

## 7d. Ask User to Re-run Tests

‚Üí Use AskUserQuestion:
  - "Re-run all tests to verify fixes?"
  - "No, stop for now"

‚úì User confirms ‚Üí Proceed to step 4 (Run Tests)  
‚úó User declines ‚Üí Proceed to step 8  

## 8. Completion

‚Üí Check if all originally-failing tests were fixed:
  - If yes ‚Üí Display: "‚úÖ All tests fixed and passed!"
  - If no ‚Üí Display: "‚ö†Ô∏è Workflow incomplete. Some tests remain unfixed."

‚Üí Show summary:
  - Tests fixed in this session
  - Tests skipped/remaining
  - Root causes addressed

‚Üí Clear todo list with TodoWrite (empty)  
‚Üí Exit  

---

**‚ö†Ô∏è  CRITICAL EXECUTION RULES**

- **Silent execution**: NEVER use `tee` when running build or test commands. Redirect all output to log files (`> "$LOG_FILE" 2>&1`). Only inspect logs when command returns non-zero exit code.
- **Exit code checking**: Always capture and check exit codes to resolve build and test success/failure. Zero = success, non-zero = failure.
- **No assumptions**: Never assume errors are "pre-existing" or skip investigating them. All errors must be analyzed unless user explicitly stops the workflow.
- **No Git Commits:** DO NOT commit changes as part of this workflow. Users will do that separately.
